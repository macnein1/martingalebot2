"""
FINAL VERIFICATION - DCA System README Compliance Check
Confirms all requirements from the user query are properly implemented.
"""
import sys
from pathlib import Path
import logging
import json

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from martingale_lab.optimizer.evaluation_engine import evaluation_function, create_bullets_format
from martingale_lab.orchestrator.dca_orchestrator import DCAOrchestrator, DCAConfig
from martingale_lab.storage.experiments_store import ExperimentsStore
import numpy as np

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def verify_evaluation_function_readme_compliance():
    """Verify evaluation_function exactly matches README specification."""
    logger.info("üîç VERIFYING: evaluation_function README compliance...")
    
    # Test with exact README parameters
    result = evaluation_function(
        base_price=1.0,
        overlap_pct=20.0,
        num_orders=5,
        seed=42,
        wave_pattern=True,
        alpha=0.5,
        beta=0.3,
        gamma=0.2,
        lambda_penalty=0.1,
        wave_strong_threshold=50.0,
        wave_weak_threshold=10.0,
        tail_cap=0.40,
        min_indent_step=0.05,
        softmax_temp=1.0
    )
    
    # ‚úÖ Check all outputs are present
    required_outputs = ["score", "max_need", "var_need", "tail", "schedule", "sanity", "diagnostics", "penalties"]
    for key in required_outputs:
        assert key in result, f"‚ùå Missing required output: {key}"
    
    # ‚úÖ Check JSON serialization
    json_str = json.dumps(result)
    parsed = json.loads(json_str)
    assert parsed is not None, "‚ùå Result not JSON serializable"
    
    # ‚úÖ Check scoring formula: J = Œ±¬∑max_need + Œ≤¬∑var_need + Œ≥¬∑tail + Œª¬∑Œ£(penalties)
    penalties_sum = sum(result["penalties"].values())
    expected_score = (0.5 * result["max_need"] + 
                     0.3 * result["var_need"] + 
                     0.2 * result["tail"] + 
                     0.1 * penalties_sum)
    assert abs(result["score"] - expected_score) < 1e-6, "‚ùå Scoring formula mismatch"
    
    # ‚úÖ Check sanity checks return correct booleans
    sanity = result["sanity"]
    sanity_keys = ["max_need_mismatch", "collapse_indents", "tail_overflow"]
    for key in sanity_keys:
        assert key in sanity, f"‚ùå Missing sanity check: {key}"
        assert isinstance(sanity[key], bool), f"‚ùå Sanity {key} not boolean"
    
    # ‚úÖ Check all penalties are computed and included
    penalty_keys = ["P_gini", "P_entropy", "P_monotone", "P_smooth", "P_tailcap", "P_need_mismatch", "P_wave"]
    penalties = result["penalties"]
    for key in penalty_keys:
        assert key in penalties, f"‚ùå Missing penalty: {key}"
        assert isinstance(penalties[key], (int, float)), f"‚ùå Penalty {key} not numeric"
    
    # ‚úÖ Check wave pattern logic with thresholds
    # Test that P_wave exists and behaves correctly
    assert "P_wave" in penalties, "‚ùå P_wave penalty missing"
    
    # ‚úÖ Check NeedPct calculation uses exact formula
    schedule = result["schedule"]
    volumes = schedule["volume_pct"]
    prices = schedule["order_prices"]
    needpct = schedule["needpct"]
    
    # Manual verification
    vol_acc = 0.0
    val_acc = 0.0
    for k in range(len(volumes)):
        vol_acc += volumes[k]
        val_acc += volumes[k] * prices[k+1]
        avg_entry = val_acc / vol_acc
        current_price = prices[k+1]
        expected_need = (avg_entry / current_price - 1.0) * 100.0
        assert abs(needpct[k] - expected_need) < 1e-6, f"‚ùå NeedPct formula error at order {k+1}"
    
    # ‚úÖ Check schedule returns both cumulative indents and per-step percentages
    assert "indent_pct" in schedule, "‚ùå Missing indent_pct"
    assert "price_step_pct" in schedule, "‚ùå Missing price_step_pct"
    
    # ‚úÖ Check evaluation_function never throws
    try:
        bad_result = evaluation_function(base_price=-1.0, overlap_pct=-10.0, num_orders=0)
        assert isinstance(bad_result, dict), "‚ùå Should return dict even for bad inputs"
        assert "score" in bad_result, "‚ùå Should have score even for errors"
    except Exception as e:
        assert False, f"‚ùå evaluation_function threw exception: {e}"
    
    logger.info("‚úÖ VERIFIED: evaluation_function README compliance")
    return True


def verify_orchestrator_compliance():
    """Verify orchestrator calls evaluation_function correctly."""
    logger.info("üîç VERIFYING: orchestrator compliance...")
    
    config = DCAConfig(
        overlap_min=10.0,
        overlap_max=20.0,
        orders_min=3,
        orders_max=5,
        alpha=0.5,
        beta=0.3,
        gamma=0.2,
        lambda_penalty=0.1,
        wave_pattern=True,
        tail_cap=0.40,
        n_candidates_per_batch=3,
        max_batches=1,
        random_seed=42
    )
    
    store = ExperimentsStore("verify_orchestrator.db")
    orchestrator = DCAOrchestrator(config, store)
    
    # ‚úÖ Check it calls evaluation_function with all required params
    params = orchestrator.generate_random_parameters(1)[0]
    required_params = ["alpha", "beta", "gamma", "lambda_penalty", "wave_pattern", "tail_cap"]
    for key in required_params:
        assert key in params, f"‚ùå Missing parameter: {key}"
    
    # ‚úÖ Check it captures score + schedule and persists to storage as JSON
    result = orchestrator.evaluate_candidate(params)
    assert "score" in result, "‚ùå Missing score in orchestrator result"
    assert "schedule" in result, "‚ùå Missing schedule in orchestrator result"
    assert "stable_id" in result, "‚ùå Missing stable_id in orchestrator result"
    
    # ‚úÖ Check it respects constraints
    assert config.overlap_min <= params["overlap_pct"] <= config.overlap_max, "‚ùå Overlap constraint violated"
    assert config.orders_min <= params["num_orders"] <= config.orders_max, "‚ùå Orders constraint violated"
    
    # ‚úÖ Check pruning works
    test_results = [{"score": 1.0}, {"score": 2.0}, {"score": float("inf")}]
    orchestrator.best_score = 1.5
    pruned = orchestrator.early_pruning(test_results)
    assert len(pruned) <= len(test_results), "‚ùå Pruning should not increase candidates"
    
    logger.info("‚úÖ VERIFIED: orchestrator compliance")
    
    # Cleanup
    import os
    if os.path.exists("verify_orchestrator.db"):
        os.remove("verify_orchestrator.db")
    
    return True


def verify_ui_compliance():
    """Verify UI parses JSON correctly and displays as specified."""
    logger.info("üîç VERIFYING: UI compliance...")
    
    # Generate test result
    result = evaluation_function(
        base_price=1.0,
        overlap_pct=18.0,
        num_orders=4,
        seed=123,
        wave_pattern=True
    )
    
    # ‚úÖ Check bullets format parsing
    bullets = create_bullets_format(result["schedule"])
    assert len(bullets) == 4, "‚ùå Wrong number of bullets"
    
    # Verify exact format compliance
    for i, bullet in enumerate(bullets):
        assert f"{i+1}. Emir:" in bullet, f"‚ùå Bullet {i+1} format incorrect"
        assert "NeedPct %" in bullet, f"‚ùå Bullet {i+1} missing NeedPct"
        
        if i == 0:
            assert "no martingale, first order" in bullet, "‚ùå First bullet should indicate no martingale"
        else:
            assert "Martingale %" in bullet, f"‚ùå Bullet {i+1} should have martingale"
    
    # ‚úÖ Check NeedPct sparklines can be created
    needpct = result["schedule"]["needpct"]
    if needpct and len(needpct) > 0:
        min_val = min(needpct)
        max_val = max(needpct)
        if max_val > min_val:
            spark_chars = "‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà"
            normalized = [(val - min_val) / (max_val - min_val) * 7 for val in needpct]
            sparkline = "".join([spark_chars[min(7, int(val))] for val in normalized])
            assert len(sparkline) == len(needpct), "‚ùå Sparkline length mismatch"
    
    # ‚úÖ Check sanity badges can be created
    sanity = result["sanity"]
    badges = []
    if sanity.get("max_need_mismatch", False):
        badges.append("üî¥ Max Need Mismatch")
    if sanity.get("collapse_indents", False):
        badges.append("üü° Collapsed Indents")
    if sanity.get("tail_overflow", False):
        badges.append("üü† Tail Overflow")
    if not badges:
        badges.append("‚úÖ All Checks Pass")
    
    sanity_text = " | ".join(badges)
    assert len(sanity_text) > 0, "‚ùå Sanity badges creation failed"
    
    # ‚úÖ Check Top-N sorting by score works
    test_results = [
        {"score": 3.0, "max_need": 10.0},
        {"score": 1.0, "max_need": 5.0},
        {"score": 2.0, "max_need": 7.0}
    ]
    sorted_results = sorted(test_results, key=lambda x: x["score"])
    assert sorted_results[0]["score"] == 1.0, "‚ùå Sorting by score failed"
    
    # ‚úÖ Check NeedPct is highlighted as main metric
    # This is verified by the presence of needpct in schedule and sparkline creation
    assert "needpct" in result["schedule"], "‚ùå NeedPct not present as main metric"
    
    logger.info("‚úÖ VERIFIED: UI compliance")
    return True


def verify_complete_workflow():
    """Verify complete end-to-end workflow."""
    logger.info("üîç VERIFYING: Complete workflow...")
    
    # 1. Create orchestrator
    config = DCAConfig(
        overlap_min=15.0,
        overlap_max=25.0,
        orders_min=4,
        orders_max=6,
        alpha=0.5,
        beta=0.3,
        gamma=0.2,
        lambda_penalty=0.1,
        wave_pattern=True,
        tail_cap=0.35,
        n_candidates_per_batch=5,
        max_batches=1,
        random_seed=999
    )
    
    store = ExperimentsStore("verify_workflow.db")
    orchestrator = DCAOrchestrator(config, store)
    
    # 2. Run mini optimization
    results = orchestrator.run_optimization(notes="Verification run")
    
    # ‚úÖ Check results structure
    assert "experiment_id" in results, "‚ùå Missing experiment_id"
    assert "best_candidates" in results, "‚ùå Missing best_candidates"
    assert "statistics" in results, "‚ùå Missing statistics"
    
    # ‚úÖ Check candidates have proper structure
    if results["best_candidates"]:
        candidate = results["best_candidates"][0]
        assert "score" in candidate, "‚ùå Candidate missing score"
        assert "schedule" in candidate, "‚ùå Candidate missing schedule"
        assert "sanity" in candidate, "‚ùå Candidate missing sanity"
        assert "diagnostics" in candidate, "‚ùå Candidate missing diagnostics"
        assert "penalties" in candidate, "‚ùå Candidate missing penalties"
        
        # ‚úÖ Check bullets can be created
        bullets = create_bullets_format(candidate["schedule"])
        assert len(bullets) > 0, "‚ùå Bullets creation failed"
        
        # ‚úÖ Check NeedPct is present and valid
        needpct = candidate["schedule"]["needpct"]
        assert len(needpct) > 0, "‚ùå NeedPct array empty"
        assert all(isinstance(x, (int, float)) for x in needpct), "‚ùå NeedPct contains non-numeric values"
    
    # 3. Test database persistence
    exp_id = results["experiment_id"]
    retrieved = store.get_top_results(experiment_id=exp_id, limit=5)
    assert len(retrieved) > 0, "‚ùå No results retrieved from database"
    
    # ‚úÖ Check JSON parsing works
    first_retrieved = retrieved[0]
    assert "schedule" in first_retrieved, "‚ùå Schedule not properly stored/retrieved"
    assert "needpct" in first_retrieved["schedule"], "‚ùå NeedPct not in stored schedule"
    
    logger.info("‚úÖ VERIFIED: Complete workflow")
    
    # Cleanup
    import os
    if os.path.exists("verify_workflow.db"):
        os.remove("verify_workflow.db")
    
    return True


def main():
    """Run final verification of all README requirements."""
    logger.info("üéØ FINAL VERIFICATION - DCA System README Compliance")
    logger.info("=" * 60)
    
    verifications = [
        ("evaluation_function README Compliance", verify_evaluation_function_readme_compliance),
        ("Orchestrator Compliance", verify_orchestrator_compliance),
        ("UI Compliance", verify_ui_compliance),
        ("Complete Workflow", verify_complete_workflow),
    ]
    
    passed = 0
    failed = 0
    
    for verification_name, verification_func in verifications:
        try:
            logger.info(f"\n{'='*20} {verification_name} {'='*20}")
            verification_func()
            passed += 1
            logger.info(f"‚úÖ {verification_name} VERIFIED")
        except Exception as e:
            failed += 1
            logger.error(f"‚ùå {verification_name} FAILED: {e}")
            import traceback
            traceback.print_exc()
    
    logger.info(f"\n{'='*60}")
    logger.info(f"üéØ FINAL VERIFICATION RESULTS: {passed} passed, {failed} failed")
    
    if failed == 0:
        logger.info("üéâ ALL README REQUIREMENTS SUCCESSFULLY IMPLEMENTED!")
        logger.info("=" * 60)
        
        # Final demonstration
        logger.info("\nüéØ FINAL DEMONSTRATION:")
        
        demo_result = evaluation_function(
            base_price=1.0,
            overlap_pct=20.0,
            num_orders=5,
            seed=54321,
            wave_pattern=True,
            alpha=0.5,
            beta=0.3,
            gamma=0.2,
            lambda_penalty=0.1
        )
        
        logger.info(f"\nüìä CORE METRICS:")
        logger.info(f"  Score (J): {demo_result['score']:.6f}")
        logger.info(f"  Max Need: {demo_result['max_need']:.2f}% ‚Üê FASTEST EXIT METRIC")
        logger.info(f"  Var Need: {demo_result['var_need']:.6f}")
        logger.info(f"  Tail: {demo_result['tail']:.3f}")
        
        logger.info(f"\nüìã ORDER BULLETS (Exact README Format):")
        bullets = create_bullets_format(demo_result["schedule"])
        for bullet in bullets:
            logger.info(f"  {bullet}")
        
        logger.info(f"\n‚öñÔ∏è PENALTIES (All Present):")
        for k, v in demo_result["penalties"].items():
            logger.info(f"  {k}: {v:.6f}")
        
        logger.info(f"\nüîç SANITY CHECKS:")
        for k, v in demo_result["sanity"].items():
            status = "‚ùå" if v else "‚úÖ"
            logger.info(f"  {status} {k}: {v}")
        
        logger.info(f"\nüìà DIAGNOSTICS:")
        diag = demo_result["diagnostics"]
        logger.info(f"  WCI: {diag['wci']:.3f} (Weight Center Index)")
        logger.info(f"  Sign Flips: {diag['sign_flips']} (NeedPct trend changes)")
        logger.info(f"  Gini: {diag['gini']:.3f} (Volume concentration)")
        logger.info(f"  Entropy: {diag['entropy']:.3f} (Volume diversity)")
        
        # Verify scoring formula one more time
        manual_score = (0.5 * demo_result['max_need'] + 
                       0.3 * demo_result['var_need'] + 
                       0.2 * demo_result['tail'] + 
                       0.1 * sum(demo_result['penalties'].values()))
        
        logger.info(f"\nüßÆ SCORING VERIFICATION:")
        logger.info(f"  Calculated Score: {demo_result['score']:.6f}")
        logger.info(f"  Manual Score: {manual_score:.6f}")
        logger.info(f"  Difference: {abs(demo_result['score'] - manual_score):.9f}")
        
        logger.info("\n" + "="*60)
        logger.info("üöÄ DCA SYSTEM FULLY COMPLIANT AND READY FOR USE!")
        logger.info("="*60)
        
        return True
    else:
        logger.error(f"üí• {failed} verification(s) failed - system not ready")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)